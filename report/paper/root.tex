\documentclass[letterpaper,10 pt,conference]{ieeeconf}

\IEEEoverridecommandlockouts
\overrideIEEEmargins

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{mathtools}
\usepackage[backend=bibtex8,style=ieee,sorting=none]{biblatex}

\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{caption}
\usepackage{subcaption}
\usepackage{standalone}
\usepackage{tikz}
\usepackage{tikzscale}
\usetikzlibrary{calc}

\graphicspath{{img/}}
\addbibresource{references.bib}


\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\mat}[1]{\mathbf{#1}}

\title{\LARGE \bf
  Optimizing Topometric Maps for Teach and Repeat
}


\author{David Landry and Alexandre Gari\'epy}


\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract} Topometric maps are a convenient approach to record reference
points (also called anchor points) in the Teach and Repeat paradigm. When using
such a map, the question of how sparsely the environment should be sampled
arises. A very dense sampling will be more robust at the cost of an increased
database size. Fewer anchor points will make the use of loop-closure algorithms
easier. To replace the traditional approach of recording data at a regular
intervals based on the odometry of the wheels, we propose an offline algorithm
that automatically filters the anchor points points in a way that guarantees a
certain tolerance to error. The topometric map then contains the minimal amount
of nodes required to execute the \textit{repeat} runs, making loop-closure
easier and allowing the mapping of longer trajectories. Offline experiments
confirmed the ability of our approach to optimize topometric maps, and showed
that the size of the map will vary given a certain demanded error tolerance.
\end{abstract}

\section{INTRODUCTION}

Teach and Repeat is a popular and efficient paradigm to execute tasks where it
is possible to have an operator show the robot how to execute it beforehand. It
has been implemented using stereo cameras \cite{Furgale10} or lidar sensors
\cite{Sprunk13}. As shown in \cite{Furgale10}, an hybrid between metric and
topographic maps is an appropriate way to record a series of anchor points
during a \textit{teach} run. We will refer to those maps as \textit{topometric}.
One of the questions that arises when implementing such a navigation algorithm
is how sparsely should the environment be mapped. A very frequent sampling, with
lots of \textit{anchor points} to localize against during the \textit{repeat}
run, will provide more robustness to the algorithm. However the robustness comes
at the cost of a very large database size. The database size is in itself a
burden but more importantly if affects the performance of an eventual loop
closure algorithm.

The current approach leaves it to the operator to decide how frequently an
anchor point is recorded. Both in \cite{Furgale10} and \cite{Sprunk13} the robot
is told to record a new anchor point at regular intervals, based on the odometry
of the wheels. This approach comes with no consideration for the complexity of
the environment, and the operator has to guess what parameter would be
appropriate for a given application.

Inspired by \cite{Churchill15}, we will implement an algorithm such that there
is no need for this guesswork. By carefully evaluating the ability of the anchor
points to localize the robot on the taught trajectory, we will be able to remove
the unnecessary ones while still guaranteeing a certain tolerance to error
during the \textit{repeat} phase. Instead of choosing at what frequency the
environment is mapped, the operator will now have to wonder what tolerance to
error he needs, which is a more natural decision to make given a certain
application.


\section{PROBLEM DEFINITION}

As in \cite{Sprunk13} our, implementation of the Teach and Repeat paradigm uses
maps that contain anchor points, transformations to go from one anchor to
another, as well as recorded commands for the whole trajectory. This paper is
focused on improving the number of anchor points that are needed to reliably
repeat the taught trajectory. We will focus on the underlying topometric map,
which is a graph where the nodes are the anchor points (which are point clouds
in our case), and the edges are the transformations between neighboring anchor
points.

We begin with an unoptimized topometric map, containing $N$, a set of $i$
interconnected nodes named $n_1, n_2, ..., n_{i}$, each of the $n_x$
representing a point cloud. The transformations are named $\mat{T}_1,
\mat{T}_2,.. \mat{T}_{i-1}$. $\prescript{y}{x}{\mat{T}}$ is the transformation
from frame $x$ to $y$ and is shorthand for $\mat{T}_y
\mat{T}_{y-1}...\mat{T}_{x+1} \mat{T}_x$

The nodes in $N$ are in fact anchor points to our teach an
repeat implementation. They are point clouds in this case, but could be any mean
of localization, like the features of an image. Our objective is to find the
smallest subset of $N$ such that the robot can reliably localize itself against
an anchor point at any time while it follows the taught trajectory.

More formally, we are looking for $S \in \mathcal{P}(N)$ containing $j \leq i$
nodes $s$ such that $\forall x \in \{ 0..j-1 \}$ we have
$\texttt{LOCALISES}(s_x, s_{x+1})$. We can think of the \texttt{LOCALISES}
predicate as an indication that it is possible for a robot receiving the reading
$s_{x+1}$ during a repeat phase to localize using $s_{x}$ as a reference.
Obviously \texttt{LOCALISES} is an ideal function that is difficult to obtain in
a real environment. 

With that in mind, and by making the assumption that localization is easier if
you are closer to your reference point, having \texttt{LOCALISES} for all the
pairs of successive nodes means that a robot could use $S$ as the nodes of its
map and still be able to localize at any point during the repeat phase.
Obviously, it is difficult to create such a function in a real environment.
Using ICP we will approximate \texttt{LOCALISES}. This process is described in
more detail in section \ref{ssec:localises}.

\section{OUR APPROACH}
\label{approach}

\subsection{The \texttt{LOCALISES} predicate}
\label{ssec:localises}

We want to approximate a method $\texttt{LOCALISES}(n_k, n_l)$ that tells us, given two
nodes in our teach and repeat map, if node $n_k$ can be used as a reference
point if we were to receive the point cloud of $n_l$ as a reading during the
repeat run. In our context, having a successful ICP is important for accurate
localization. Our main idea is to affirm that we can be confident that localization is
possible if we have a good tolerance to error: that is, the point cloud pair can
do reliable ICP even if the initial estimates are inaccurate. We will simulate
ICP localization given various induced errors, and make sure that the ICP
algorithm converges to a transformation equivalent to the transformation we
obtained using our best estimate. 

More formally, we simulate $\texttt{LOCALISES}(n_x, n_y)$ with 

\begin{align}
  \| \texttt{ICP}&(n_x, n_y, \mat{E}(u,v) \prescript{y}{x}{\mat{T}}) - 
\texttt{ICP}(n_x, n_y, \prescript{y}{x}{ \mat{T} }) \| < \epsilon  \\ 
 &\forall u \in [0, 2 \pi),  v \in [0, \pi] \\
\end{align}

$\texttt{ICP}(n_x, n_y, \mat{T})$ returns a transformation that minimizes the
distance between the pairs of closest points in the point clouds of $n_x$ and
$n_y$. $\mat{T}$ is the initial estimate we have of this transformation. Usually
it can be something similar to an odometry estimate.

$\mat{E}(u,v)$ is a function we use to output the transformation matrix of an
induced error. $u$ and $v$ are the parameters of an ellipsoid defined like so:

\begin{align}
  \begin{split}
x &= a \cos u \sin v \\
y &= b \cos u \cos b \\
z &= c \cos v \\
\end{split}
\end{align}

As this is only an approximation, it is important to use the same algorithms
that are mounted on the robot, such that a successful localization in the
simulation is likely to be repeated experimentally, and vice versa.

Our implementation uses the Frobenius*** norm for this computation, and $\epsilon$
was set to $0.15$ for our experiments, after some trial and error.

\begin{figure}[thpb]
  \centering
  \includegraphics[scale=0.4]{convergence_bassin}
  \caption{The covergence bassin of a pair of readings}
  \label{convergence_bassin}
\end{figure}

\subsection{Finding the smallest topometric map}
\label{matching-readings} 

We are now ready to compute the smallest topometric map. To do so we iterate on
the anchor points of the map. For each node $n_i$ we run
$\texttt{LOCALISES}(n_i,n_j)$ with $j = i + 2$, then $i + 3$... until we find a
$k$ such that $\texttt{LOCALISES}(n_i, n_k)$ is false.

The results of this search are represented as an oriented graph, where an edge
from $n_{i}$ to $n_{j}$ means that we have $\texttt{LOCALISES}(n_i, n_j)$ An
example graph is represented on figure \ref{graph_unoptimized}.


\begin{figure}[thpb]
  \centering
  \includegraphics[scale=1.0]{unoptimized-graph}
  \caption{Representing convergence ellipses as an oriented graph}
  \label{graph_unoptimized}
\end{figure}

After computing the oriented graph, we can finally look for the smallest subset
of nodes such that we can localize ourself from one end of the map to another.

The graph reprensentation presented in the section \ref{matching-readings} allows to easily solve
this problem. If each edge on the graph has a cost of 1, the minimal topometric map from the
beginning of the map (node $b$) to the end of the map (node $e$) can be found using the Dijkstra's
algorithm \cite{dijkstra}. The nodes used on the shortest path from $b$ to $e$ are the minimal node
subset of the optimal topometric map. Note that the way the graph is built, it is guaranteed to be
acyclic, therefore Dijkstra's algorithm will always find the minimal node subset.


\begin{figure}[thpb]
  \centering
  \includegraphics[scale=1.0]{optimized-graph}
  \caption{Optimal node subset using the graph approach and Dijkstra's algorithm}
\end{figure}


\section{EXPERIMENTS}

\subsection{Map optimization}

\begin{figure}
  \centering
  \includegraphics[scale=0.4]{map_optimization}
  \caption{Comparison of un-optimized and optimized maps}
\end{figure}

First, we implemented our topometric map optimization method as described in section \ref{approach}.
Then, we optimized three dataset acquired during three previous \textit{teach} sessions. The
\textit{terasse} dataset is a short run paving stones on Université Laval's campus. It was a busy
summer day, and passer-by that were curious about the robot introduced noise in the point
clouds. The \textit{forest} dataset is a short run in some woodland on campus, making the environment
highly unstructured. Finally, we used \textit{hallway}, mapping an indoor hallway, as a
benchmark through the development of the algorithm. It records the robot going in a straight line.
During this \textit{teach} session the robot was tuned to record a very high number of point clouds,
so we expect the optimization algorithm to filter a lot of nodes in this case. All three datasets
were collecting using a Clearpath Husky A200 equiped with a Velodyne HDL-32e lidar sensor. The teach
sessions were executed using home-grown Teach and Repeat software.

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{hallway_optimization}
  \caption{Optimization of hallway dataset}
\end{figure}

We used parameters $a=0.4 m$, $b=0.4 m$ and $\epsilon=0.05 m$ for those optimization runs. The
optimization is indeed able to reduce the number of point clouds necessary to execute a
\textit{repeat} session. As expected, depending on the robot trajectory some zones contain more
superfluous nodes than others. Straight lines in very static environments tend to require less
anchor points, where trajectories containing heavy rotations are less filtered out. The algorithm
tends not only to eliminate superfluous point clouds, but it also gets rid of the nodes that are
hard to localize against, for instance a point cloud containing a passer-by that is not there on the
other neighboring point clouds. For instance in the \textit{hallway} dataset, even though the
overall localization using ICP is easy, some clouds could not be used for localization at all.
Finally, the number of remaining nodes could be tuned down even more, at the cost of a lower
tolerance to error. Our algorithm thus gives the user a very direct way to control the
memory-size/ability-to-localize tradeoff. The numbers given in table
\ref{tabopti} are meant to give a general idea, but they could be very different depending on the
geometry of the environment, the parameters given by the user, and the quality of the odometry
estimates.

We noticed that in some datasets, the \textit{ICP chain} used to reconstruct the relative positions
between the nodes may be noisier than we thought, yielding unrealistic positions for sucessive
anchor points. It leaves us wondering wether we should rather rely on the odometry from the weels as
a reference transformation. A Kalmann Filter using both odometry and the results
from ICP would improve this situation.

\begin{table}[h]
\centering
\begin{tabular}{|l|r|r|r|}
  \hline
Dataset & N. of nodes & N. of nodes (optimized) & \% nodes kept \\
\hline
  hallway & 117 & 14 & 12.0 \\
\hline
terasse & 304 & 186 & 61.2 \\
  \hline
forest & 62 & 41 & 66.1 \\
  \hline
\end{tabular}
\caption{The result of the optimization on different datasets}
\label{tabopti}
\end{table}


\subsection{Parameters $a$ and $b$}

We observed the effects of parameters $a$ and $b$ when optimizing the \textit{forest} dataset. As
expected, the percentage of nodes kepts after optimization grows with the tolerance ellipse. We
noticed that very small tolerance ellipses does not necessarily mean a very small amount of nodes;
that is explained by the quality of the anchor points themselves. A point cloud that has a very 
initial position estimate, or that contains a passer-by, is always going to make the optimization
more difficult, no matter how small the tolerance ellipse is.

\begin{table}[h]
\centering
\begin{tabular}{|r|r|r|r|}
  \hline
a (m) & b (m) &  N. of nodes after optimization & \% of points kept \\
\hline
  0.2 & 0.1 & 41 & 66.1  \\
\hline
 0.4 & 0.4 & 41 & 66.1 \\
  \hline
 1.6 & 0.8 & 44 & 72.1 \\
  \hline
1.8 & 0.9 & 59 & 96.7 \\
  \hline
  2.0 & 1.0 & 61 & 100.0 \\
  \hline
\end{tabular}
\caption{The effects of parameters $a$ and $b$}
\label{tabopti}
\end{table}

\section{FUTURE WORK}

\subsection{Experiment with Husky} In the future, we would like to validate
experimentally, using a Clearpath Husky Robot, that the produced topometric map
can indeed be used for localization. To reach that goal, we would optimize maps
acquired during the teach phase of a teach and repeat. Then, we would compare
the accuracy of the robot during the repeat phase using the optimized and
non-optimized maps.

We would also want to validate experimentally the effect of the parameters $a$ and $b$ (the semi-axis of the ellipse)
on the accuracy of the map. That is, given a certain repeat run, we would like to validate that if
we optimized the map demanding a tolerance to error $x$, we are able to move the robot outside of
its trajectory by $x$ and still be able to localize.

Finally, we would like to validate if removing nodes from the topometric map harms precision in highly unstructured environment like forests.

\subsection{Multi-objective optimization for automatic parameter selection}

To focus our efforts on fiding the minimal node subset, we made the simplification
the convergence ellipse has a fixed sized. In other words, we manually specify
the parameters $a$ and $b$ for which we want the ICP to converge. With those
parameters given, we can reduce the problem to a finding the shortest path in a
graph, as shown in section \ref{approach}.

A limitation of this approach is that, if for a given node, decreasing the size of the ellipse would
make the ICP converge on further nodes of the graph. We can see that as a tradeoff between the
precision and the number of nodes of the final map.

An improvement on the current method would be to model the problem as a multi-objective optimization
problem. Given an initial ellipse size for each node of the graph, reducing the size of the
ellipse has a cost. The goal is to find the best tradeoff between the length of the shortest path in
the graph and the the costs of the nodes that are used by that shortest path.

\subsection{Reoptimization of the localizability graph}

The optimization could be taken a step further if we make one more assumption about the behavior
of the \textit{repeat} phase. So far we assumed that the repeat algorithm would work that way: when
following a trajectory passing by anchor points $a$ then $b$, the robot would use point $a$ as a
reference for localization until it is very close to $b$, and then switch reference. It would be
reasonable to have the robot behave differently and make it switch references somewhere in between
$a$ and $b$, which would make the localization easier.

This modification would allow us to make an even better optimization of the topometric maps. When
finding the shortest path in the localizability graph, we ensure that every node in the graph can be
reached by using the preceding node as a reference. This means (assuming that the ICP algorithm is
reflexive) that every second anchor point in our optimized map becomes superfluous and can be removed.

\section{Conclusion}

We introduced an offline algorithm that can greatly reduce the amount of reference points needed in
a topometric map for it to be used in a Teach and Repeat implementation. Our algorithm computes the
ability of candidate anchor points to be used as good reference when navigating a given
environment. The results of this computation are used to filter out nodes in the topometric map
until it is as small as possible to execute a \textit{repeat} run, given a demanded tolerance to
error. Our approach was tested using 3d point clouds from a lidar sensor and ICP, but is general
enough to be applied to vision-based implementations of the Teach and Repeat paradigm. Experiments
confirm that the algorithm is indeed able to optimize topometric maps, and that the user can tune
the size of the map by changing his error tolerance requirements.

\printbibliography

\end{document}
